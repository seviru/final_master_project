###########################################################################################################
### In this folder we have various scripts and subfolders where we will store plenty of data regarding	###
### the parsing of our MongoDB to extract specific information from gene clusters.			###
###########################################################################################################

cluster_finder.py: It's a python script which querys to search all the clusters with more than 10 unigenes,
		and save all of the cluster names in a specified file.
		USAGE: $python cluster_finder.py > <OUTFILE>
		NEEDED: Need python 3.7. If It fails, try: $module load Python/3.7.2-GCCcore-8.2.0

cluster_finder.launcher.sh: It's a bash script used to run our cluster_finder.py script in a computation cluster
		environment.
		USAGE: (Inside the cluster)$sbatch -o <OUTPUTFILE> -t <HOURS:MINS:SECS DEDICATED TO THE TASK> cluster_finder.launcher.sh

cluster_info_retriever.py: It's a python script which for a given input cluster name will search for information
		regarding all the unigenes It contains, creating a FASTA (.fas) file with the sequence of all
		the unigenes as well as the best protein hit from swissprot, and a TABLE (.tsv) file with the
		best hit for each unigene found. The FASTA file will be stored in a new created folder called
		"fastas" and the TABLE file will be stored in a new created folder called "tables".
		USAGE: $python cluster_info_retriever.py <CLUSTER_NAME>
		NEEDED: Need Python 3.7. If It fails, try: $module load Python/3.7.2-GCCcore-8.2.0

cluster_info_retriever.launcher.sh: It's a script used to run our script cluster_info_retriever.py in a computation
		cluster environment.
		USAGE: (Inside the cluster)$sbatch -a 1-<CLUSTER FILE NUMBER OF LINES> -o <SLURM OUT FOLDER>/%A_%a.out -e <SLURM ERROR FOLDER>/%A_%a.err -t <HOURS:MINS:SECS DEDICATED TO THE JOB> cluster_info_retriever.launcher.sh <FILENAME WITH THE CLUSTER NAMES, A.K.A. <OUTFILE>.txt>
		NEEDED: Already existing folders <SLURM OUT FOLDER> and <SLURM ERROR FOLDER>

sbatch_chunks.sh: It's a script used to run more than 200.000 jobs at the same time in our slurm computational
		cluster. It's a little more complex to call than the other ones, since It needs more variables.
		NEEDED: <CHUNKSIZE>: Ammount of jobs that we want to send as if they were the same one.
			<RECORDS>: Number of individual jobs we wouls launch if we didnt group them. $(wc -l ../data/<OUTFILE>.txt | awk '{print $1});
			<JOBS>: Number of jobs we will launch once we have grouped them. $(((records/chunksize)+1));
		USAGE: $sbatch -a 1-${<JOBS>} -t <TIME> -o <SLURM OUT FOLDER>/%A_%a.out -e <SLURM EROR FOLDER>/%A_%a.err ./cluster_info_retriever.sbatch_chunks.sh ../data/<OUTFILE>.txt ${<CHUNKSIZE>} ${<RECORDS>} cluster_info_retriever.launcher.sh 

<OUTFILE>.txt: We will have a .txt file in this folder containing the clusters list which meet the requirements of
		the "cluster_finder.py" script and the partition they will go in the next step.

./<SLURM OUT FOLDER>/: Folder where we will store the output from running in batch our script cluster_info_retriever.launcher.sh . In our case is ./slurm_out/

./<SLURM ERROR FOLDER>/: Folder where we will store the errors from running in batch our script cluster_info_retriever.launcher.sh . In our case is ./slurm_err/

cluster_list.py: It's a script which for a given cluster size It outputs all the cluster names that match at least that size.
		USAGE:$python clusters_list.py <CLUSTER MINIMUM SIZE>

cluster_info_retriever.cheatsheet.sh: Instructions on how to use cluster_info_retriever.sbatch_chunks.sh in more detail.